{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neox.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZjT10VSxRFmR4/WpkbPGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hnipun/ColabProjects/blob/master/neox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fwNzVjhb7O2",
        "outputId": "4231f644-c3ba-44b2-be00-4e5342c5f08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: labml in /usr/local/lib/python3.7/dist-packages (0.4.146)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml) (3.13)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml) (3.1.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml) (1.21.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install labml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import labml.utils.download\n",
        "from labml import lab\n",
        "from labml import logger, monit\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ndubx8RPcfOZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "o4R5I_U1cTQ4",
        "outputId": "b2318a13-1238-4485-f266-6700872b120e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">Download<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t581.12ms</span>\n",
              "</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 128\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        labml.utils.download.download_file(\n",
        "            'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt',\n",
        "            lab.get_data_path() / 'tiny_shakespeare.txt',\n",
        "        )\n",
        "        with open(lab.get_data_path() / 'tiny_shakespeare.txt', \"r\") as f:\n",
        "            self.text = f.read()\n",
        "        self.text = list(self.text)\n",
        "        self.vocab = {t: ids for ids, t in enumerate(set(self.text))}\n",
        "        self.text_tensor = torch.tensor(self._to_vocab(self.text))\n",
        "\n",
        "    def _to_vocab(self, text):\n",
        "        res = []\n",
        "\n",
        "        for t in text:\n",
        "            res.append(self.vocab[t])\n",
        "\n",
        "        return res\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) // SEQ_LENGTH\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.text_tensor[SEQ_LENGTH * idx:SEQ_LENGTH * (idx + 1)]\n"
      ],
      "metadata": {
        "id": "HQGD8MAve-NZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctd = CustomTextDataset()\n",
        "train_dataloader = DataLoader(CustomTextDataset(), batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "NDXjP2-hiARV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srr8PP8ZTnuX",
        "outputId": "6850da92-8e3c-4ecc-901b-28d729925c98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2, 12,  7,  ..., 34, 18, 34],\n",
              "        [40, 34, 59,  ..., 58, 29,  1],\n",
              "        [54, 53, 30,  ..., 45,  1, 59],\n",
              "        ...,\n",
              "        [ 0, 55, 45,  ..., 59, 59, 32],\n",
              "        [36, 12,  7,  ..., 51, 45,  0],\n",
              "        [64, 45,  0,  ...,  0, 11, 39]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "GUM7WLhdQbPI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EMBEDDINGS = 65\n",
        "EMBEDDING_DIM = 16\n",
        "N_TRANSOFRMER_LAYERS = 3\n",
        "\n",
        "\n",
        "NUM_ATTENTION_HEADS = 8\n",
        "NUM_FEATURES = 32"
      ],
      "metadata": {
        "id": "8o7PBcIOZH01"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "print(device)\n",
        "\n",
        "def get_mask(seq_length): # write a test for mask\n",
        "  # batch_size, seq_length, seq_length, heads is enough\n",
        "  # 1, seq_length, seq_length, 1 is enough\n",
        "  a = torch.ones(seq_length, seq_length, dtype=bool)\n",
        "  a = torch.triu(a, diagonal=1) \n",
        "  a = torch.unsqueeze(a, 0)\n",
        "  a = torch.unsqueeze(a, -1)\n",
        "\n",
        "  return a\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.norm_layer = nn.LayerNorm(EMBEDDING_DIM)\n",
        "    \n",
        "    self.key_linear = torch.nn.Linear(EMBEDDING_DIM, NUM_ATTENTION_HEADS * NUM_FEATURES)\n",
        "    self.query_linear = torch.nn.Linear(EMBEDDING_DIM, NUM_ATTENTION_HEADS * NUM_FEATURES)\n",
        "    self.value_linear = torch.nn.Linear(EMBEDDING_DIM, NUM_ATTENTION_HEADS * NUM_FEATURES)\n",
        "\n",
        "    self.softmax = torch.nn.Softmax(dim=2)\n",
        "    self.linear_layer =  torch.nn.Linear(NUM_ATTENTION_HEADS * NUM_FEATURES, EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_residual = x\n",
        "    x = self.norm_layer(x)\n",
        "    seq_length = x.shape[1]\n",
        "    kx = self.key_linear(x)\n",
        "    # logger.log(f'kx layer {kx.size()}')\n",
        "    kx = kx.view(BATCH_SIZE,seq_length , NUM_ATTENTION_HEADS, NUM_FEATURES)\n",
        "    # logger.log(f'kx layer {kx.size()}')\n",
        "\n",
        "    qx = self.query_linear(x)\n",
        "    qx = kx.view(BATCH_SIZE, seq_length, NUM_ATTENTION_HEADS, NUM_FEATURES)\n",
        "\n",
        "    vx = self.value_linear(x)\n",
        "    vx = kx.view(BATCH_SIZE, seq_length, NUM_ATTENTION_HEADS, NUM_FEATURES)\n",
        "\n",
        "    score = torch.einsum('bihd,bjhd ->bijh', qx, kx) #is this correct ?\n",
        "    score = score/(NUM_FEATURES)**0.5\n",
        "\n",
        "    # logger.log(f'score layer {score.size()}')\n",
        "\n",
        "    mask = get_mask(seq_length).to(x.device)\n",
        "    score = score.masked_fill(mask, float(\"-inf\"))\n",
        "\n",
        "    probs = self.softmax(score)\n",
        "\n",
        "    # logger.log(f'probs layer {probs.size()}')\n",
        "\n",
        "    output = torch.einsum('bijh,bjhd ->bihd', probs, vx)\n",
        "    # logger.log(f'output layer {output.size()}')\n",
        "    output = output.reshape(BATCH_SIZE, seq_length, NUM_ATTENTION_HEADS * NUM_FEATURES)\n",
        "    # logger.log(f'output layer {output.size()}')\n",
        "    output = self.linear_layer(output)\n",
        "    # logger.log(f'output layer {output.size()}')\n",
        "\n",
        "    # logger.log(f'kx layer {kx.size()}')\n",
        "\n",
        "    return x + x_residual\n",
        "\n",
        "\n",
        "class FeedForwardLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.norm_layer = nn.LayerNorm(EMBEDDING_DIM)\n",
        "    self.linear_layer1 =  torch.nn.Linear(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_DIM/2)\n",
        "    self.activation_layer = nn.ReLU()\n",
        "    self.linear_layer2 =  torch.nn.Linear(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_residual = x\n",
        "    x = self.norm_layer(x)\n",
        "    x = self.linear_layer1(x)\n",
        "    x = self.activation_layer(x)\n",
        "    x = self.linear_layer2(x)\n",
        "\n",
        "    return x + x_residual\n",
        "\n",
        "\n",
        "class TrnsformerLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.multi_head_attention = MultiHeadAttentionLayer()\n",
        "    self.feed_forward_layer = FeedForwardLayer()\n",
        "\n",
        "  def forward(self, x): # (batch_size, seq_length, emd_dim)\n",
        "    x = self.multi_head_attention(x)\n",
        "    # logger.log(f'multihead attention layer {x.size()}')\n",
        "    return self.feed_forward_layer(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em9iEeAzREuA",
        "outputId": "18dc6c63-6f31-4c51-819a-50f50dced667"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trsnsformer(nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embeddings = torch.nn.Embedding(NUM_EMBEDDINGS, EMBEDDING_DIM)\n",
        "        self.transormer_layers = nn.ModuleList([TrnsformerLayer()] * N_TRANSOFRMER_LAYERS)\n",
        "        self.readout_layer = torch.nn.Linear(EMBEDDING_DIM, NUM_EMBEDDINGS)\n",
        "\n",
        "  def forward(self, x):\n",
        "        # logger.log(f'input {x.size()}') # (batch_size, seq_length)\n",
        "        x = self.embeddings(x)\n",
        "        # logger.log(f'embedding layer {x.size()}') # (batch_size, seq_length, emd_dim)\n",
        "        for transormer_layer in self.transormer_layers:\n",
        "          x = transormer_layer(x)\n",
        "        # logger.log(f'transormer layer {x.size()}') # (batch_size, seq_length, emd_dim)\n",
        "\n",
        "        return self.readout_layer(x) # (batch_size, seq_length, vocab_size)"
      ],
      "metadata": {
        "id": "ev_bocZ6RJ8e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Trsnsformer()\n",
        "t(next(iter(train_dataloader))).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnitUQmGThMV",
        "outputId": "e2c07e24-971d-47bc-e78c-a23d2f86bd34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 128, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Trsnsformer()\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "for epoch in monit.loop(NUM_EPOCHS):\n",
        "    for train_data in monit.iterate('train', train_dataloader):\n",
        "      train_data = train_data.to(device)\n",
        "\n",
        "      if train_data.size()[0] < 64:\n",
        "        continue\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      out = model(train_data[:,:-1])\n",
        "      out = out.to(device)\n",
        "\n",
        "      loss = F.cross_entropy(out.view(BATCH_SIZE*127, 65),train_data[:,1:].reshape(BATCH_SIZE*127))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    logger.log(str(loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "bdIOm1C2CRZ8",
        "outputId": "ad30796b-f508-4199-f541-77a9472a879f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-75e61ecb801d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrsnsformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-cef8b8f8476f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EMBEDDINGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransormer_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrnsformerLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN_TRANSOFRMER_LAYERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadout_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EMBEDDINGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5d09dbcd4864>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_head_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttentionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForwardLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# (batch_size, seq_length, emd_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5d09dbcd4864>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFeedForwardLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layer1\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                     raise AttributeError(\n\u001b[0;32m-> 1207\u001b[0;31m                         \"cannot assign module before Module.__init__() call\")\n\u001b[0m\u001b[1;32m   1208\u001b[0m                 \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = next(iter(train_dataloader))\n",
        "print(d.size())\n",
        "print(d[:,:-1].size())\n",
        "print(d[:,1:].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTGfI036J74k",
        "outputId": "5ea4a3d1-9961-47b3-a23c-9736e4387bb1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 128])\n",
            "torch.Size([64, 127])\n",
            "torch.Size([64, 127])\n"
          ]
        }
      ]
    }
  ]
}