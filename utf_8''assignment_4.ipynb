{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "cse598",
      "graded_item_id": "fUHpL",
      "launcher_item_id": "6uwHL"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "utf-8''assignment_4.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hnipun/ColabProjects/blob/master/utf_8''assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAjr0FlEKkTk",
        "colab_type": "text"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TjKw_OJKkTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAME = \"Nipun Wijerathne\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZjPl0w1KkTp",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6eee44dabe633069a7f8ed70b4b476a2",
          "grade": false,
          "grade_id": "cell-5666377cfb23173a",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "nQcbcK3_KkTq",
        "colab_type": "text"
      },
      "source": [
        "## Generative Adversarial Network\n",
        "\n",
        "Generative Adversarial Networks a.k.a GANs are emerging techniques that model high dimensional distribution of data. They achieve this by training a pair of networks, Generator and Discriminator, with competing loss terms. As an analogy we can think of these models as an art forger and the other being an art expert. In GAN literature we term Generator as the forger and the duty of the Generator is to produce fake images(forgeries) to deceive the art expert(Discriminator). The Discriminator which receives both the real images and fake images tries to distinguish between them and find the fake images. Both are trained simulataneously and are always in competition with each other. This competition between the Generator and Discriminator drives them to improve their models continuously. The models are trained until the Generator produces the fake images that are indistinguishable from the real images.  <br>\n",
        "\n",
        "In this setup, the Generator does not have access to the real images whereas the Discriminator has access to both the real and the generated fake images. \n",
        "\n",
        "Let us define Discriminator D that takes image as input and produces a number **(0/1)** as output and a Generator G that takes random noise as input and outputs a fake image. In practice, G and D are trained alternately i.e., For a fixed generator G, the Discriminator D is trained to classify the training data as real(output a value close to 1) or fake(output a value close to 0). In the next step we freeze the Discriminator and we train the Generator G to produce a image(fake) that outputs a value close to 1(real) when passed through the Discriminator D. Thus, if the Generator is perfectly trained then the Discriminator D will be maximally confused by the images generated by G and predict 0.5 for all the inputs. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3338c48de2966aa172ea652b81e859ab",
          "grade": false,
          "grade_id": "cell-5aa13c2371a1a79f",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "4W3DDx-QKkTr",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, we will try to implement a Generative Adversarial Network on MNIST data and generate images that resemble the digits from the MNIST dataset.\n",
        "\n",
        "To implement a GAN, we basically require 5 components:\n",
        "\n",
        "- Real Dataset (real distribution)\n",
        "- Low dimensional random noise that is input to the Generator to produce fake images\n",
        "- Generator that generates fake images\n",
        "- Discriminator that acts as an expert to distinguish real and fake images.\n",
        "- Training loop where the competition occurs and models better themselves.\n",
        "\n",
        "\n",
        "Let us implement each of the parts and train the overall model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8d46140b6ea3595cba72cc5508570673",
          "grade": false,
          "grade_id": "cell-6e76e79150210ee7",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "OuejHurhKkTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3c7c6a1-85b1-407f-90cc-e07ec8eeeba1"
      },
      "source": [
        "## import packages\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "\n",
        "\n",
        "## Checks for the availability of GPU \n",
        "if torch.cuda.is_available():\n",
        "    print(\"working on gpu!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No gpu! only cpu ;)\")\n",
        "    device = 'cpu'\n",
        "    \n",
        "## The following random seeds are just for deterministic behaviour of the code and evaluation\n",
        "\n",
        "##############################################################################\n",
        "################### DO NOT MODIFY THE CODE BELOW #############################    \n",
        "##############################################################################\n",
        "\n",
        "if device == 'cpu':    \n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "elif device == 'cuda':\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "############################################################################### "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No gpu! only cpu ;)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b4271785c2da30ad8eeaf6c7577ac462",
          "grade": false,
          "grade_id": "cell-4476cbd0147a9ca1",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "NoVyIxXqKkTu",
        "colab_type": "text"
      },
      "source": [
        "# Step -2\n",
        "\n",
        "In this step we work on preparing the data. We normalize the images to range [-1, +1] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e803a09e3d78811959df8e7bd1abc5d8",
          "grade": false,
          "grade_id": "cell-1253959e710a3e17",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "eOuXxlLWKkTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "325b2c18-c35b-4873-eab8-b0ec0093262c"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('./data'):\n",
        "    os.mkdir('./data')\n",
        "root = './data/'\n",
        "\n",
        "train_bs = 128\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.5],\n",
        "                                std=[0.5])\n",
        "        ])\n",
        "\n",
        "training_data = torchvision.datasets.MNIST(root, train=True, transform=transform,download=True)\n",
        "train_loader=torch.utils.data.DataLoader(dataset=training_data, batch_size=train_bs, shuffle=True, drop_last=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8523026.02it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 130514.63it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2125173.92it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 49019.51it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYHa2pNKkTx",
        "colab_type": "text"
      },
      "source": [
        "Let us define a function which takes (batchsize, dimension) as input and returns a random noise of requested dimensions. This noise tensor will be the input to the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdzHukNVKkTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noise(bs, dim):\n",
        "    \"\"\"Generate random Gaussian noise.\n",
        "    \n",
        "    Inputs:\n",
        "    - bs: integer giving the batch size of noise to generate.\n",
        "    - dim: integer giving the dimension of the the noise to generate.\n",
        "    \n",
        "    Returns:\n",
        "    A PyTorch Tensor containing Gaussian noise with shape [bs, dim]\n",
        "    \"\"\"\n",
        "    \n",
        "    out = (torch.randn((bs, dim))).to(device)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZs_aGwKkTz",
        "colab_type": "text"
      },
      "source": [
        "Generator architecture:\n",
        "\n",
        "\n",
        "- noise_dim -> 256\n",
        "- LeakyReLU (works well for the Generators)\n",
        "- 256 -> 512\n",
        "- LeakyReLU\n",
        "- 512 -> 1024\n",
        "- LeakyReLU\n",
        "- 1024 -> out_size(784)\n",
        "- TanH \n",
        "\n",
        "- LeakyRELU: https://pytorch.org/docs/stable/nn.html#leakyrelu \n",
        "- Fully connected layer: https://pytorch.org/docs/stable/nn.html#linear \n",
        "- TanH activation: https://pytorch.org/docs/stable/nn.html#tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "84f4bba5dfd066531a02aff46903e631",
          "grade": false,
          "grade_id": "cell-7cc3bc10411dcfef",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6NMeyabLKkT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim=100, out_size=784):\n",
        "        super(Generator, self).__init__()\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        REST OF THE MODEL HERE\n",
        "        \n",
        "        # define a fully connected layer (self.layer1) from noise_dim -> 256 neurons\n",
        "        \n",
        "        # define a leaky relu layer(self.leaky_relu) with negative slope=0.2.\n",
        "        \n",
        "        # define a fully connected layer (self.layer2) from 256 -> 512 neurons\n",
        "        \n",
        "        # define a fully connected layer (self.layer3) from 512 -> 1024 neurons\n",
        "                \n",
        "        # define a fully connected layer (self.layer4) from 1024 -> out_size neurons\n",
        "        \n",
        "        # define a tanh activation function (self.tanh)\n",
        "        \n",
        "        '''\n",
        "        self.layer1     =  nn.Linear(noise_dim, 256)\n",
        "        self.leaky_relu =  nn.LeakyReLU(0.2)\n",
        "        self.layer2     =  nn.Linear(256, 512)\n",
        "        self.layer3     =  nn.Linear(512, 1024)\n",
        "        self.layer4     =  nn.Linear(1024, out_size)\n",
        "        self.tanh       =  nn.Tanh()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        '''\n",
        "        \n",
        "        CODE HERE\n",
        "        \n",
        "        Make a forward pass of the input through the generator. Leaky relu is used as the activation \n",
        "        function in all the intermediate layers. Tanh activation function is only used at the end (which\n",
        "        means only after self.layer4)\n",
        "        \n",
        "        Note that, generator takes an random noise as input and gives out fake \"images\". Hence, the output \n",
        "        after tanh activation function is reshaped into the same size as the real images. i.e., \n",
        "        [batch_size, n_channels, H, W] == (batch_size, 1,28,28)\n",
        "         \n",
        "        '''\n",
        "        layers = nn.Sequential(\n",
        "            self.layer1,   \n",
        "            self.leaky_relu,\n",
        "            self.layer2, \n",
        "            self.leaky_relu,\n",
        "            self.layer3,\n",
        "            self.leaky_relu,\n",
        "            self.layer4,\n",
        "            self.tanh\n",
        "        )\n",
        "\n",
        "        x    = layers(x)\n",
        "        N, I = x.size()\n",
        "        x    = x.view(N, 1, 28, 28)\n",
        "        \n",
        "        return x\n",
        "             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7c56b9fe39557c952bbbeff278c7af7a",
          "grade": false,
          "grade_id": "cell-45a730ceed00661f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Alp4z8o8KkT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8bea28858ae33797ffe8e77faa419058",
          "grade": true,
          "grade_id": "cell-1201781994aa660e",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "DMY3DiPEKkT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRBoBWpLKkT6",
        "colab_type": "text"
      },
      "source": [
        "Discriminator architecture:\n",
        "\n",
        "- input_size->512\n",
        "- LeakyReLU with negative slope = 0.2\n",
        "- 512 -> 256\n",
        "- LeakyReLU with negative slope = 0.2\n",
        "- 256->1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "34eec526c9bb46aa81265f0c806b3036",
          "grade": false,
          "grade_id": "cell-35c45e3c98ee6b3c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mULEQp5RKkT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Similar to the Generator, we now define a Discriminator which takes in a vector and output a single scalar \n",
        "## value. \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=784):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        REST OF THE MODEL HERE\n",
        "        \n",
        "        # define a fully connected layer (self.layer1) from input_size -> 512 neurons\n",
        "                \n",
        "        # define a leaky relu layer(self.leaky_relu) with negative slope=0.2.\n",
        "        \n",
        "        # define a fully connected layer (self.layer2) from 512 -> 256 neurons\n",
        "        \n",
        "        # define a fully connected layer (self.layer3) from 256 -> 1 neurons\n",
        "        \n",
        "        '''\n",
        "        self.layer1     =  nn.Linear(input_size, 512)\n",
        "        self.leaky_relu =  nn.LeakyReLU(0.2)\n",
        "        self.layer2     =  nn.Linear(512, 256)\n",
        "        self.layer3     =  nn.Linear(256, 1)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        CODE HERE\n",
        "        \n",
        "        The Discriminator takes a vectorized input of the real and generated fake images. Reshape the input \n",
        "        to match the Discriminator architecture. \n",
        "        \n",
        "        Make a forward pass of the input through the Discriminator and return the scalar output of the \n",
        "        Discriminator.\n",
        "       \n",
        "        '''\n",
        "        N, C, H, W = x.size()\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        layers = nn.Sequential(\n",
        "            self.layer1,   \n",
        "            self.leaky_relu,\n",
        "            self.layer2, \n",
        "            self.leaky_relu,\n",
        "            self.layer3,\n",
        "        )\n",
        "        y    = layers(x)\n",
        "        \n",
        "        return y       \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj6vojyTKkT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator()\n",
        "discriminator = discriminator.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c3aa2f551fa8ae9186778fd4506e923d",
          "grade": true,
          "grade_id": "cell-a7d6ce029d371d29",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "C85x3treKkT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tvielE4KkT_",
        "colab_type": "text"
      },
      "source": [
        "Binary cross entropy loss function. The loss function includes sigmoid activation followed by logistic loss.\n",
        "\n",
        "Binary cross entropy loss with logits: https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YssjazcQKkUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bce_loss = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "982781bc5ffe42637aee8f9441ae1472",
          "grade": false,
          "grade_id": "cell-c2effd2a3a92e314",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "qSeBFMBIKkUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DLoss(logits_real, logits_fake, targets_real, targets_fake):\n",
        "    # YOUR CODE HERE\n",
        "    # raise NotImplementedError()\n",
        "    '''\n",
        "    d1 - binary cross entropy loss between outputs of the Discriminator with real images \n",
        "         (logits_real) and targets_real.\n",
        "         \n",
        "    d2 - binary cross entropy loss between outputs of the Discriminator with the generated fake images \n",
        "         (logits_fake) and targets_fake.\n",
        "    \n",
        "    '''\n",
        "    d1 = bce_loss(logits_real, targets_real)\n",
        "    d2 = bce_loss(logits_fake, targets_fake)\n",
        "\n",
        "    total_loss = d1 + d2\n",
        "    return total_loss\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "22323b7a9977e1f785763ffe4ea48005",
          "grade": true,
          "grade_id": "cell-5e663605cae71ddd",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "lG1HP0vVKkUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aa415eb21546673fda360c87f3371724",
          "grade": false,
          "grade_id": "cell-65c511b43cc5b4cd",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "ZjFvdI02KkUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GLoss(logits_fake, targets_real):\n",
        "    # YOUR CODE HERE\n",
        "    # raise NotImplementedError()\n",
        "    \n",
        "    '''\n",
        "    The aim of the Generator is to fool the Discriminator into \"thinking\" the generated images are real.\n",
        "    \n",
        "    g_loss - binary cross entropy loss between the outputs of the Discriminator with the generated fake images \n",
        "         (logits_fake) and targets_real.\n",
        "         \n",
        "    Thus, the gradients estimated with the above loss corresponds to generator producing fake images that \n",
        "    fool the discriminator.\n",
        "    \n",
        "    '''\n",
        "    g_loss = bce_loss(logits_fake, targets_real)\n",
        "\n",
        "    return g_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "122f6bce71cd3dca653a4be39ded4e26",
          "grade": true,
          "grade_id": "cell-38199fa74255aa47",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "56t7nvDHKkUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iazeeQmDKkUI",
        "colab_type": "text"
      },
      "source": [
        "Optimizers for training the Generator and the Discriminator.\n",
        "\n",
        "Adam optimizer: https://pytorch.org/docs/stable/optim.html#torch.optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p47xe2AbKkUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "epochs = 50\n",
        "noise_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ed0830fb787517e296a291241b8c4651",
          "grade": false,
          "grade_id": "cell-71045021080a4282",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3jw5w5UHKkUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "501f4255-dac8-4dd1-fbaa-b193c4d42390"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        \n",
        "        # We set targets_real and targets_fake to non-binary values(soft and noisy labels).\n",
        "        # This is a hack for stable training of GAN's.  \n",
        "        # GAN hacks: https://github.com/soumith/ganhacks#6-use-soft-and-noisy-labels\n",
        "        \n",
        "        targets_real = (torch.FloatTensor(images.size(0), 1).uniform_(0.8, 1.0)).to(device)\n",
        "        targets_fake = (torch.FloatTensor(images.size(0), 1).uniform_(0.0, 0.2)).to(device)\n",
        "        \n",
        "        images = images.to(device)\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        \n",
        "        \n",
        "        ## D-STEP:\n",
        "        ## First, clear the gradients of the Discriminator optimizer.\n",
        "        optimizer_D.zero_grad()\n",
        "        ## Estimate logits_real by passing images through the Discriminator\n",
        "        logits_real = discriminator(images)\n",
        "        ## Generate fake_images by passing random noise through the Generator. Also, .detach() the fake images \n",
        "        ## as we don't compute the gradients of the Generator when optimizing Discriminator.\n",
        "        ## fake_images = generator(noise(train_bs, noise_dim)).detach()\n",
        "        fake_images = generator(noise(train_bs, noise_dim)).detach()\n",
        "        ## Estimate logits_fake by passing the fake images through the Discriminator\n",
        "        logits_fake = discriminator(fake_images)\n",
        "        ## Compute the Discriminator loss by calling DLoss function.\n",
        "        discriminator_loss = DLoss(logits_real, logits_fake, targets_real, targets_fake)\n",
        "        ## Compute the gradients by backpropagating through the computational graph. \n",
        "        discriminator_loss.backward() \n",
        "        ## Update the Discriminator parameters.\n",
        "        optimizer_D.step()\n",
        "        \n",
        "        \n",
        "        ## G-STEP:\n",
        "        ## clear the gradients of the Generator. \n",
        "        optimizer_G.step()\n",
        "        ## Generate fake images by passing random noise through the Generator. \n",
        "        fake_images = generator(noise(train_bs, noise_dim))\n",
        "        ## Estimate logits_fake by passing the fake images through the Discriminator.\n",
        "        logits_fake = discriminator(fake_images)\n",
        "        ## compute the Generator loss by caling GLoss.\n",
        "        generator_loss = GLoss(logits_fake, targets_real)\n",
        "        ## compute the gradients by backpropagating through the computational graph.\n",
        "        generator_loss.backward() \n",
        "        ## Update the Generator parameters. \n",
        "        optimizer_G.step()\n",
        "    \n",
        "    print(\"D Loss: \", discriminator_loss.item())\n",
        "    print(\"G Loss: \", generator_loss.item())\n",
        "          \n",
        "    if epoch % 2 == 0:\n",
        "        viz_batch = fake_images.data.cpu().numpy()\n",
        "        fig = plt.figure(figsize=(8,10))\n",
        "        for i in np.arange(1, 10):\n",
        "            ax = fig.add_subplot(3, 3, i)\n",
        "            img = viz_batch[i].squeeze()\n",
        "            plt.imshow(img)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D Loss:  0.6383984684944153\n",
            "G Loss:  2.0689663887023926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAIkCAYAAADGRfiqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAA0QAMGAKABGjAAAA3QgAEAaOD/A5vUPnhNveUzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x720 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "D Loss:  0.6249721646308899\n",
            "G Loss:  2.139434814453125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}